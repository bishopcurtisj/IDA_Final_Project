{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "superbowl = pd.read_csv('..\\\\data\\\\nfl_halftime_wide.csv')\n",
    "superbowl=superbowl['Date']\n",
    "superbowl = pd.to_datetime(superbowl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sign in to spotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bisho\\.conda\\envs\\IDA\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: use options instead of chrome_options\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "soups=[]\n",
    "\n",
    "options = Options()\n",
    "options.binary_location = \"C:\\Program Files\\chrome-win64\\chrome.exe\"\n",
    "driver=webdriver.Chrome(chrome_options=options,executable_path=\"C:\\Program Files\\chromedriver-win64\\chromedriver.exe\")\n",
    "\n",
    "driver.get(\"https://accounts.spotify.com/en/login?continue=https%3A%2F%2Fcharts.spotify.com/login\")\n",
    "username = driver.find_element_by_id(\"login-username\")\n",
    "password = driver.find_element_by_id(\"login-password\")\n",
    "\n",
    "username.send_keys(\"username\")\n",
    "password.send_keys(\"password\")\n",
    "\n",
    "driver.find_element_by_id(\"login-button\").click()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape spotify charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_soups=[]\n",
    "\n",
    "for sb in superbowl[-8:]:\n",
    "    driver.get('https://charts.spotify.com/charts/view/regional-us-weekly/'+str(sb.year)+'-'+str(sb.month).zfill(2)+'-'+str(sb.day+4).zfill(2))\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    song_soups.append(bs(page_source, 'html.parser'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "soups=[]\n",
    "\n",
    "for sb in superbowl[-3:]:\n",
    "    driver.get('https://charts.spotify.com/charts/view/artist-us-weekly/'+str(sb.year)+'-'+str(sb.month).zfill(2)+'-'+str(sb.day+4).zfill(2))\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    soups.append(bs(page_source, 'html.parser'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse soup and save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parse artist html soup\n",
    "\n",
    "rank=[]\n",
    "name = []\n",
    "peak = []\n",
    "previous = []\n",
    "streak = []\n",
    "year = []\n",
    "\n",
    "yr=0\n",
    "for soup in soups:\n",
    "    i=0\n",
    "    for row in soup.find_all('tr'):\n",
    "        if i ==1:    \n",
    "            rank.append(row.find('span', attrs={'aria-label':\"Current position\"}))\n",
    "            name.append(row.find('span', {\"class\":\"styled__StyledTruncatedTitle-sc-135veyd-22 kKOJRc\"}))\n",
    "            peak.append(row.find('td', {\"class\":\"TableCell__TableCellElement-sc-1nn7cfv-0 dLdEGj styled__RightTableCell-sc-135veyd-4 kGfYTK\"}))\n",
    "            previous.append(row.find('td', {\"class\":\"TableCell__TableCellElement-sc-1nn7cfv-0 dLdEGj styled__RightTableCell-sc-135veyd-4 kGfYTK\"}))\n",
    "            streak.append(row.find('td', {\"class\":\"TableCell__TableCellElement-sc-1nn7cfv-0 dLdEGj styled__RightTableCell-sc-135veyd-4 kGfYTK\"}))\n",
    "            year.append(2022+yr)\n",
    "        else:\n",
    "            i=1\n",
    "    \n",
    "    yr+=1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists= pd.DataFrame({'year':year,'rank':rank,'name':name,'peak': peak, 'previous':previous, 'streak':streak})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in artists.columns:\n",
    "    if col != 'year':\n",
    "        artists[col] = artists[col].apply(lambda x: x.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists.to_csv('..\\\\data\\\\artists.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parse html soup\n",
    "\n",
    "rank=[]\n",
    "name = []\n",
    "peak = []\n",
    "previous = []\n",
    "streak = []\n",
    "year = []\n",
    "title=[]\n",
    "streams = []\n",
    "\n",
    "yr=0\n",
    "for soup in song_soups:\n",
    "    i=0\n",
    "    for row in soup.find_all('tr'):\n",
    "        if i ==1:    \n",
    "            rank.append(row.find('span', attrs={'aria-label':\"Current position\"}))\n",
    "            name.append(row.find('a', {'class':\"styled__StyledHyperlink-sc-135veyd-25 bVVLJU\"}))\n",
    "            title.append(row.find('span', {\"class\":\"styled__StyledTruncatedTitle-sc-135veyd-22 kKOJRc\"}))\n",
    "            peak.append(row.find('td', {\"class\":\"TableCell__TableCellElement-sc-1nn7cfv-0 dLdEGj styled__RightTableCell-sc-135veyd-4 kGfYTK\"}))\n",
    "            previous.append(row.find('td', {\"class\":\"TableCell__TableCellElement-sc-1nn7cfv-0 dLdEGj styled__RightTableCell-sc-135veyd-4 kGfYTK\"}))\n",
    "            streak.append(row.find('td', {\"class\":\"TableCell__TableCellElement-sc-1nn7cfv-0 dLdEGj styled__RightTableCell-sc-135veyd-4 kGfYTK\"}))\n",
    "            streams.append(row.find('td', {\"class\":\"TableCell__TableCellElement-sc-1nn7cfv-0 kJgiFu styled__RightTableCell-sc-135veyd-4 kGfYTK\"}))\n",
    "            year.append(2017+yr)\n",
    "        else:\n",
    "            i=1\n",
    "    \n",
    "    yr+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = pd.DataFrame({'year':year,'rank':rank,'song':title,'name':name,'peak': peak, 'previous':previous, 'streak':streak, 'streams':streams})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in songs.columns:\n",
    "    if col != 'year':\n",
    "        songs[col] = songs[col].apply(lambda x: x.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs.to_csv('..\\\\data\\\\songs.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape UK charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## no data prior to 2016\n",
    "soups=[]\n",
    "\n",
    "\n",
    "for sb in superbowl[-9:]:\n",
    "    page = requests.get('https://www.officialcharts.com/charts/albums-sales-chart/'+str(sb.year)+str(sb.month)+str(sb.day)+'/7511/')\n",
    "    soup = bs(page.content, 'html.parser')  \n",
    "    soups.append(soup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parse the data\n",
    "\n",
    "rank = []\n",
    "title = []\n",
    "artist = []\n",
    "previous = []\n",
    "peak = []\n",
    "weeks = []\n",
    "year = []\n",
    "i=0\n",
    "for soup in soups:\n",
    "    year.extend([2016+i]*100)\n",
    "    for row in soup.find_all('div', {'class':'chart-item-content relative flex w-full'}):\n",
    "        rank.append(row.find('div', {'class':'position'}).text[7:])\n",
    "        t = (row.find('a', {'class':'chart-name font-bold inline-block'}).text)\n",
    "        if t[:3]=='New':\n",
    "            title.append(t[3:])\n",
    "        else:\n",
    "            title.append(t)\n",
    "        artist.append(row.find('a', {'class':'chart-artist text-lg inline-block'}).text)\n",
    "        previous.append(row.find('li', {'class':'movement px-2 py-1 rounded-md inline-block mr-1 sm:mr-2'}).text[4:].replace(',',''))\n",
    "        peak.append(row.find('li', {'class':'peak px-2 py-1 rounded-md inline-block mr-1 sm:mr-2'}).text[6:].replace(',',''))\n",
    "        weeks.append(row.find('li', {'class':'weeks px-2 py-1 rounded-md inline-block mr-1 sm:mr-2'}).text[7:].replace(',',''))\n",
    "    i+=1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data to dataframe and csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_charts = pd.DataFrame({'year':year,'rank':rank,'title':title,'artist':artist,'previous':previous,'peak':peak,'weeks':weeks})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_charts.to_csv('..\\\\data\\\\uk_charts.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape US Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = pd.read_csv('..\\\\references\\\\wayback.csv')\n",
    "soups={}\n",
    "\n",
    "for iter, row in pages.iterrows():\n",
    "    page = requests.get(row['link'])\n",
    "    soups[row['date']]=bs(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save scraped web data as html to parse later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_html(html, path):\n",
    "    with open(path, 'wb') as f:\n",
    "        f.write(html)\n",
    "\n",
    "paths = []\n",
    "for key in soups.keys():\n",
    "    k=key.replace('/','_')\n",
    "    path = '..\\\\references\\\\html\\\\'+k+'.html'\n",
    "    save_html(bs.encode(soups[key]), path)\n",
    "    paths.append(path)\n",
    "\n",
    "pages['path']=paths\n",
    "pages.to_csv('..\\\\references\\\\wayback.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load scraped html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = pd.read_csv('..\\\\references\\\\wayback.csv')\n",
    "soups = {}\n",
    "for index, row in pages.iterrows():\n",
    "    with open(row['path'], 'rb') as f:\n",
    "        soups[row['date']] = bs(f, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse webpage content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous = []\n",
    "rank = []\n",
    "artist = []\n",
    "label = []\n",
    "total = []\n",
    "change = []\n",
    "albums = []\n",
    "date=[]\n",
    "tea = []\n",
    "sea = []\n",
    "switch = False\n",
    "\n",
    "for key in soups.keys():\n",
    "    soup = soups[key]\n",
    "    date.extend([key]*50)\n",
    "    if key == \"2/1/2019\":\n",
    "        switch = True\n",
    "    if switch:\n",
    "        \n",
    "        for rows in soup.find_all('tr', {'class':['hits_album_chart_header_full_alt2','hits_album_chart_header_full_alt1',]}):\n",
    "            previous.append(rows.find('td', {'class':'hits_album_chart_full_tw'}))\n",
    "            rank.append(rows.find('td', {'class':'hits_album_chart_full_lw'}))\n",
    "            artist.append(rows.find('span', {'class':'hits_album_chart_item_details_full_artist'}))\n",
    "            label.append(rows.find('span', {'class':'hits_album_chart_item_details_full_label'}))\n",
    "            total.append(rows.find('td', {'class':['hits_album_chart_item_top_details_full_sales chart_tweak col_sales','hits_album_chart_item_details_full_sales chart_tweak col_sales']}))\n",
    "            change.append(rows.find('td', {'class':['hits_album_chart_item_top_details_full_change chart_tweak col_change','hits_album_chart_item_details_full_change chart_tweak col_change']}))\n",
    "            albums.append(rows.find('td', {'class':['hits_album_chart_item_top_details_full_sales_albums chart_tweak col_albums','hits_album_chart_item_details_full_sales_albums chart_tweak col_albums']}))\n",
    "            tea.append(rows.find('td', {'class':['hits_album_chart_item_top_details_full_sales_tea chart_tweak col_tea','hits_album_chart_item_details_full_sales_tea chart_tweak col_tea']}))\n",
    "            sea.append(rows.find('td', {'class':['hits_album_chart_item_top_details_full_sales_sea chart_tweak col_sea','hits_album_chart_item_details_full_sales_sea chart_tweak col_sea']}))\n",
    "\n",
    "    \n",
    "\n",
    "    else:\n",
    "        for rows in soup.find_all('tr', {'class':['hits_album_chart_header_full_alt2','hits_album_chart_header_full_alt1',]}):\n",
    "            previous.append(rows.find('td', {'class':['hits_album_chart_full_tw','hits_album_chart_tw_full_top']}))\n",
    "            rank.append(rows.find('td', {'class':['hits_album_chart_full_lw','hits_album_chart_lw_full_top']}))\n",
    "            artist.append(rows.find('span', {'class':['hits_album_chart_item_details_full_artist','hits_album_chart_item_top_full_details_artist']}))\n",
    "            label.append(rows.find('td', {'class':'hits_album_chart_item_top_details_full_label'}))\n",
    "            total.append(rows.find('td', {'class':'hits_album_chart_item_top_details_full_sales'}))\n",
    "            change.append(rows.find('td', {'class':'hits_album_chart_item_top_details_full_change'}))\n",
    "            albums.append(None)\n",
    "            tea.append(None)\n",
    "            sea.append(None)\n",
    "\n",
    "         \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save parsed data as csv and dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_charts = pd.DataFrame({'date':date,'rank':rank,'lw':previous,'artist':artist,'label':label,'total':total,'change':change,'albums':albums,'tea':tea,'sea':sea})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_charts.to_csv('..\\\\data\\\\hdd_charts.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
